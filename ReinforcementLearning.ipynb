{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a2c998d-b8e6-4232-ad0c-103bd794cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f5b7ac1-eff8-4234-a059-acbc9d364c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.array([[' ' for _ in range(3)] for _ in range(3)])\n",
    "        self.current_player = 'X'\n",
    "        self.winner = None\n",
    "\n",
    "    def print_board(self):\n",
    "        for row in self.board:\n",
    "            print('|'.join(row))\n",
    "            print('-' * 5)\n",
    "\n",
    "    def is_valid_move(self, row, col):\n",
    "        return self.board[row, col] == ' '\n",
    "\n",
    "    def make_move(self, row, col):\n",
    "        if self.is_valid_move(row, col):\n",
    "            self.board[row, col] = self.current_player\n",
    "            self.check_winner()\n",
    "            self.switch_player()\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows, columns, and diagonals for a winner\n",
    "        for i in range(3):\n",
    "            if np.all(self.board[i] == self.current_player):\n",
    "                self.winner = self.current_player\n",
    "                return\n",
    "            if np.all(self.board[:, i] == self.current_player):\n",
    "                self.winner = self.current_player\n",
    "                return\n",
    "        if np.all(np.diag(self.board) == self.current_player) or np.all(np.diag(np.fliplr(self.board)) == self.current_player):\n",
    "            self.winner = self.current_player\n",
    "\n",
    "    def is_board_full(self):\n",
    "        return np.all(self.board != ' ')\n",
    "\n",
    "    def switch_player(self):\n",
    "        self.current_player = 'O' if self.current_player == 'X' else 'X'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edd4e236-d8b2-4e59-b3d9-de5ada8e2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, epsilon=0.1, alpha=0.5, gamma=0.9):\n",
    "        self.epsilon = epsilon  # Exploration-exploitation trade-off\n",
    "        self.alpha = alpha      # Learning rate\n",
    "        self.gamma = gamma      # Discount factor\n",
    "        self.q_table = {}       # Q-value table\n",
    "\n",
    "    def get_state_key(self, state):\n",
    "        return str(state)\n",
    "\n",
    "    def choose_action(self, state, valid_actions):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore: choose a random action\n",
    "            return np.random.choice(valid_actions)\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest Q-value\n",
    "            state_key = self.get_state_key(state)\n",
    "            if state_key not in self.q_table:\n",
    "                # Initialize Q-values for the state if not present\n",
    "                self.q_table[state_key] = {action: 0 for action in valid_actions}\n",
    "            return max(self.q_table[state_key], key=self.q_table[state_key].get)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        state_key = self.get_state_key(state)\n",
    "        next_state_key = self.get_state_key(next_state)\n",
    "\n",
    "        if state_key not in self.q_table:\n",
    "            # Initialize Q-values for the state if not present\n",
    "            self.q_table[state_key] = {action: 0 for action in range(9)}\n",
    "\n",
    "        if next_state_key not in self.q_table:\n",
    "            # Initialize Q-values for the next state if not present\n",
    "            self.q_table[next_state_key] = {action: 0 for action in range(9)}\n",
    "\n",
    "        # Update Q-value using the Q-learning update rule\n",
    "        self.q_table[state_key][action] = (1 - self.alpha) * self.q_table[state_key][action] + \\\n",
    "                                          self.alpha * (reward + self.gamma * max(self.q_table[next_state_key].values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a335f19-241c-46d1-8043-af421fdb170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(agent, environment, episodes=10000):\n",
    "    for episode in range(episodes):\n",
    "        environment = TicTacToe()\n",
    "        state = environment.board.flatten()\n",
    "\n",
    "        while not environment.winner and not environment.is_board_full():\n",
    "            valid_actions = [i for i in range(9) if state[i] == ' ']\n",
    "\n",
    "            action = agent.choose_action(state, valid_actions)\n",
    "            row, col = divmod(action, 3)\n",
    "\n",
    "            environment.make_move(row, col)\n",
    "\n",
    "            next_state = environment.board.flatten()\n",
    "\n",
    "            if environment.winner:\n",
    "                reward = 1 if environment.winner == 'X' else -1\n",
    "            elif environment.is_board_full():\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00ae3131-9459-439b-a76a-5fecaf19fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(agent, environment):\n",
    "    while not environment.winner and not environment.is_board_full():\n",
    "        environment.print_board()\n",
    "\n",
    "        if environment.current_player == 'X':\n",
    "            row = int(input(\"Enter row (0-2): \"))\n",
    "            col = int(input(\"Enter column (0-2): \"))\n",
    "        else:\n",
    "            action = agent.choose_action(environment.board.flatten(), [i for i in range(9) if environment.board.flatten()[i] == ' '])\n",
    "            row, col = divmod(action, 3)\n",
    "\n",
    "        environment.make_move(row, col)\n",
    "\n",
    "    environment.print_board()\n",
    "\n",
    "    if environment.winner:\n",
    "        print(f\"{environment.winner} wins!\")\n",
    "    else:\n",
    "        print(\"It's a tie!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e12a0c-ecac-40ea-a4d9-8a1d94bae7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "env = TicTacToe()\n",
    "agent = QLearningAgent()\n",
    "train_model(agent, env)\n",
    "test_model(agent, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
